# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19LljpIVSAwZ2BU6VaF13dfqaWRhJqBvv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')


"""# Load Data"""

trippub = pd.read_csv('/content/drive/MyDrive/ML project/New Data/trippub.csv')

trippub.drop(columns=trippub.columns[0], axis=1, inplace=True)

trippub.head()

vehpub = pd.read_csv('/content/drive/MyDrive/ML project/New Data/vehpub.csv')

vehpub.drop(columns=vehpub.columns[0], axis=1, inplace=True)

# Drop VEHOWNMO (very specific to vehicles owned less than 1 year)
# Drop FEGEMPGF (Survey-related detail)
vehpub.drop(columns=['VEHOWNMO', 'FEGEMPGF'], axis=1, inplace=True)

vehpub.head()

vehpub['HFUEL'].replace(-1, 5.0, inplace = True)

vehpub.replace([-8, -9, -7, -1], np.nan, inplace = True)

vehpub.dropna(inplace = True)

# Map yes and no for URBRUR [0.0] is rural
vehpub[['URBRUR']] = vehpub[['URBRUR']].replace([2.0], [0.0])

perpub = pd.read_csv('/content/drive/MyDrive/ML project/New Data/perpub.csv')

perpub.head()

hhpub = pd.read_csv('/content/drive/MyDrive/ML project/New Data/hhpub.csv')

hhpub.drop(columns=hhpub.columns[0], axis=1, inplace=True)

hhpub.head()
hhpub.columns

hhpub.replace([-8, -9, -7, -1], np.nan, inplace = True)

hhpub.dropna(inplace = True)

# Drop SAMPSTRAT (Survey-related details)
hhpub.drop(columns=['SAMPSTRAT'], axis=1, inplace=True)

# PC, SPHONE, TAB, WALK, BIKE, CAR, TAXI, BUS, TRAIN, PARA, PRICE, PLACE, PTRANS, WEBUSE17 encode
#hhpub = pd.get_dummies(hhpub, columns = ['PC', 'SPHONE', 'TAB', 'WALK', 'BIKE', 'CAR', 'TAXI', 'BUS', 'TRAIN', 'PARA', 'PRICE', 'PLACE', 'PTRANS', 'WEBUSE17'])

"""# Merge"""

# join hhpub and perpub
hh_per = pd.merge(hhpub, perpub, on= 'HOUSEID', suffixes = ('', '_y'), how = 'inner')
hh_per.columns

# join prev table with vehpub
hh_per_veh = pd.merge(hh_per, vehpub, on = ['HOUSEID', 'PERSONID'], suffixes = ('', '_y'), how = 'inner')
hh_per_veh.columns

# join prev table (hh per and veh) to trippub
full_data = pd.merge(trippub, hh_per_veh, on = ['HOUSEID', 'PERSONID', 'VEHID'], suffixes = ('', '_y'), how = 'inner')
full_data.drop(full_data.filter(regex='_y$').columns.tolist(),axis=1, inplace=True)

# remove EDUC_01, EDUC_02, EDUC_03, EDUC_04, EDUC_05 b/c redundant
full_data = full_data.drop(['EDUC_01', 'EDUC_02', 'EDUC_03', 'EDUC_04', 'EDUC_05'], axis=1)

# ENCODE MANY VARIABLES
#full_data = pd.get_dummies(full_data, columns = ['HOMEOWN', 'HHFAMINC', 'HHSTFIPS', 'HTHTNRNT', 'HTPPOPDN', 'HTRESDN', 'HTEEMPDN', 'HBHTNRNT', 'HBPPOPDN', 'HBRESDN'])

y = full_data['HFUEL']

# non-EV (values != 3 in HFUEL) are designated as 0
y.replace([1.0,2.0,4.0,5.0,97.0], 0, inplace = True)

# EV (values = 3 in HFUEL) are designated as 1
y.replace(3.0, 1, inplace = True)

X = full_data.drop(['HOUSEID', 'PERSONID', 'VEHID', 'HFUEL'], axis = 1)
X.columns

"""# Dimensionality Reduction Using Correlation Matrix"""

#######################################################################
## Create a list of features to drop according to correlation matrix ##
#######################################################################
corr_mat = X.corr().round(2)

# https://www.geeksforgeeks.org/sort-correlation-matrix-in-python/
# Retain upper triangular values of correlation matrix and
# make Lower triangular values Null
upper_corr_mat = corr_mat.where(
    np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))
  
# Convert to 1-D series and drop Null values
unique_corr_pairs = upper_corr_mat.unstack().dropna()
  
# Sort correlation pairs
sorted_mat = unique_corr_pairs.sort_values()

# remove duplicate columns
list_corr = []
for i in range(len(sorted_mat)):
  if abs(sorted_mat[i]) >= 0.6:
    list_corr.append(sorted_mat.index[i])
drop_list = []
keep_list = []

for k in range(len(list_corr)):
  drop_list.append(list_corr[k][0])
  keep_list.append(list_corr[k][1])

X = X.drop(columns=drop_list, axis=1)

"""# Dimensionality Reduction Using RF"""

# RF
from sklearn.ensemble import RandomForestRegressor

# train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

model = RandomForestRegressor(n_estimators = 100, random_state = 42, max_depth=10)
model.fit(X_train, y_train)

###############################
## Finding the optimal alpha ##
###############################
from sklearn.linear_model import Lasso
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error

# train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

alpha = [1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005]
alpha_mse = []

### 5-Fold Cross Validation ###
for a in alpha:
    m = Lasso(alpha = a, copy_X = True, random_state = 42)
    m.fit(X_train,y_train)
    mse_list = cross_val_score(m, X_test, y_test, scoring = 'neg_mean_squared_error', cv = 5)
    avg_mse = np.sum((-1)*mse_list)/5
    alpha_mse.append(avg_mse)

min_idx = [i for i in range(len(alpha_mse)) if alpha_mse[i] == min(alpha_mse)]
best_alpha = alpha[min_idx[0]]
print('Best alpha is: ' + str(best_alpha))
print("\n")

###################################
### LASSO model with best alpha ###
###################################

### Initializing and training best model ###
best_model = Lasso(alpha = best_alpha, copy_X = True, random_state = 3)
best_model.fit(X_train,y_train)
y_pred = best_model.predict(X_test)

param = best_model.coef_

### Printing features and coefficients ###
final_feat, final_coeff = [],[]
for i in range(len(param)):
    if param[i] != 0:
        final_feat.append(X.columns[i])
        final_coeff.append(param[i])
final_feat_coeff = dict(zip(final_feat, final_coeff))
final_feat

import matplotlib.pyplot as plt
features = X.columns
importances = model.feature_importances_
indices = np.argsort(importances)[-20:]  # top 20 features
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

keep_rf = [features[i] for i in indices]
drop_indices = np.argsort(importances)[:-20]
drop_rf = [features[i] for i in drop_indices]

keep_rf

X = X.drop(columns=drop_rf, axis=1)

"""# One-Hot-Encoding"""

# Encode HOMEOWN, HHFAMINC, HHSTFIPS, HTHTNRNT, HTPPOPDN, HTRESDN, HTEEMPDN, HBHTNRNT, HBPPOPDN, HBRESDN
cat = ['HOMEOWN', 'HHFAMINC', 'HHSTFIPS', 'HTHTNRNT', 'HTPPOPDN', 'HTRESDN', 'HTEEMPDN', 'HBHTNRNT', 'HBPPOPDN', 'HBRESDN', 'PC', 'SPHONE', 'TAB', 'WALK', 'BIKE', 'CAR', 'TAXI', 'BUS', 'TRAIN', 'PARA', 'PRICE', 'PLACE', 'PTRANS', 'WEBUSE17']
cat_after_corr = list(set(cat)-set(drop_list)-set(drop_rf))
X = pd.get_dummies(X, columns = cat_after_corr)

"""# Scale"""

# train-test split (to prevent data leakage from test set)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# # Get columns where range is not 0-1 so we can scale
# max_gt_1 = X.max() > 1

# Get columns where range is not 0-1 so we can scale
#max_gt_1 = full_data[full_data.columns[~full_data.columns.isin(['HOUSEID', 'PERSONID', 'VEHID', 'HFUEL'])]].max() > 1

# cols_2_scale = list(max_gt_1[max_gt_1 == True].index)

# Columns not needed to scale
#cols = list(set(full_data[full_data.columns[~full_data.columns.isin(['HOUSEID', 'PERSONID', 'VEHID', 'HFUEL'])]]) - set(cols_2_scale))

# # Columns not needed to scale
# cols = list(set(X) - set(cols_2_scale))

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train) # fit only on X_train to avoid data leakage from X_test
scaled = scaler.transform(X)

#from sklearn.preprocessing import MinMaxScaler
#scaler = MinMaxScaler()
#scaler.fit(full_data[cols_2_scale])
#scaled = scaler.transform(full_data[cols_2_scale])

scaled_X = pd.DataFrame(scaled, columns = X.columns)

#scaled_X = pd.DataFrame(scaled, columns = cols_2_scale)
#scaled_X[cols] = full_data[cols]

# merge w houseid, personid, vehid to get rid of NaN values
scaled_X = scaled_X.copy()
scaled_X[['HOUSEID', 'PERSONID', 'VEHID']] = full_data[['HOUSEID', 'PERSONID', 'VEHID']]

# data without ID's
X = scaled_X[scaled_X.columns[~scaled_X.columns.isin(['HOUSEID', 'PERSONID', 'VEHID'])]]

"""# Export Data"""

df = pd.concat([X,y], axis = 1)
df

ev = df.to_csv("ev.csv", index = False)

EV = pd.read_csv('/content/drive/MyDrive/ML project/New Data/ev.csv')
EV.head()

EV.columns

y = EV['HFUEL']
x = EV.drop(['HFUEL'], axis=1)
ev = pd.read_csv('/content/drive/MyDrive/ML project/New Data/ev.csv')

ev.head()

X = ev.loc[:, ev. columns != 'HFUEL']

y = ev['HFUEL']

"""## SMOTE"""

from collections import Counter
xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.2, random_state=15)
Counter(ytrain)

from imblearn.over_sampling import SMOTE
smt = SMOTE()
X_train, y_train = smt.fit_resample(xtrain, ytrain)

"""# Logistic Regression"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.linear_model import LogisticRegression

C = [1, 0.1, 0.01, 0.001]
f1 = []

for i in C:
  model_lr = LogisticRegression(C = i, random_state = 0, max_iter = 800).fit(X_train, y_train)
  cv = RepeatedKFold(n_splits = 5, random_state = 2)
  search = cross_val_score(model_lr, X_train, y_train, cv = cv, scoring = 'roc_auc')
  f1.append([i, search])

# minimum_C = f1[np.argmax(f1[:,1]),0]
# print(minimum_C)
mean_f1 = []
for i in range(len(f1)):
  score = np.mean(f1[i][1])
  mean_f1.append(score)

mean_f1

np.argmax(mean_f1)

lr = LogisticRegression(C = 1, random_state = 0, max_iter = 800).fit(X_train, y_train)
y_pred = lr.predict(xtest)

from sklearn import metrics
print(metrics.classification_report(ytest, y_pred))

from sklearn.metrics import roc_auc_score
roc_auc_score(ytest, y_pred)

from sklearn.metrics import confusion_matrix
lr_conf_matrix = confusion_matrix(y_true = ytest, y_pred = y_pred)
lr_conf_matrix

"""# Decision Tree"""





"""# Support Vector Machine

"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.svm import SVC

C = [1, 0.1, 0.01, 0.001]
f1 = []

for i in C:
  model_svm = SVC(kernel ='linear',C = i, random_state = 0,max_iter = 800).fit(X_train, y_train)
  cv = RepeatedKFold(n_splits = 5, random_state = 2)
  search = cross_val_score(model_svm, X_train, y_train, cv = cv, scoring = 'f1')
  f1.append([i, search])

# minimum_C = f1[np.argmax(f1[:,1]),0]
# print(minimum_C)
mean_f1 = []
for i in range(len(f1)):
  score = np.mean(f1[i][1])
  mean_f1.append(score)

np.argmax(mean_f1)

#Linear Kernel Support Vector Machine
from sklearn.svm import SVC
svc = SVC(kernel = 'linear', C =2, random_state = 1).fit(X_train, y_train)
y_predict = svc.predict(xtest)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn import datasets, svm, metrics
svm_conf_matrix = confusion_matrix(y_true=ytest, y_pred=y_predict)
print(svm_conf_matrix)
print(metrics.classification_report(ytest, y_predict))

from sklearn.metrics import roc_auc_score
roc_auc_score(ytest, y_predict)

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.svm import SVC

C = [1, 0.1, 0.01]
gamma = [0.1, 1, 0.01]
f1 = []

for i in C:
  for j in gamma:
    model_rbf = SVC(kernel ='rbf', probability=False, C = i, gamma = j,random_state = 0,max_iter = 800).fit(X_train, y_train)
    cv = RepeatedKFold(n_splits = 5, random_state = 2)
    search = cross_val_score(model_rbf, X_train, y_train, cv = cv, scoring = 'f1')
    f1.append([i,j, search])

from sklearn.svm import SVC
svc = SVC(kernel = 'rbf', C =1, random_state = 1).fit(X_train, y_train)
y_predict = svc.predict(xtest)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn import datasets, svm, metrics
svm_conf_matrix = confusion_matrix(y_true=ytest, y_pred=y_predict)
print(svm_conf_matrix)
print(metrics.classification_report(ytest, y_predict))

from sklearn.metrics import roc_auc_score
roc_auc_score(ytest, y_predict)